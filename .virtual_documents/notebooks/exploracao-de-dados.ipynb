











import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

from ydata_profiling import ProfileReport





renda = pd.read_csv('../data/raw/previsao_de_renda.csv').drop('Unnamed: 0', axis=1)
renda.head(5)





prof = ProfileReport(renda, explorative=True, minimal=True)
prof


prof.to_file('../reports/renda_analisys.html')








id_duplicados = [id for id in renda['id_cliente'].value_counts().index if renda['id_cliente'].value_counts()[id] != 1]
count_id_Duplicados = renda.loc[renda['id_cliente'].isin(id_duplicados)].sort_values(by='id_cliente').groupby('data_ref')['id_cliente'].count()
count_id_Totais = renda.groupby('data_ref')['id_cliente'].count()

print('='*80)
print('Total de ID por data de registro na base de dados:')
print('='*80)
print(count_id_Totais)
print('='*80)
print('Total de ID duplicados por data de registro na base de dados:')
print('='*80)
print(count_id_Duplicados)
print('='*80)
print('Total de ID únicos por data de registro na base de dados:')
print('='*80)
print(count_id_Totais - count_id_Duplicados)


renda.loc[renda['id_cliente'].isin(id_duplicados)].sort_values(by='id_cliente').head(5)





renda['data_ref'] = pd.to_datetime(renda['data_ref'])
renda['data_ref']

















def calcular_entropia(tab_freq):
    h = 0
    for classe in tab_freq.index:
        h += tab_freq[classe] * np.log2(tab_freq[classe])
    return h * -1

def calcular_entropia_maxima(tab_freq):
    return np.log2(len(tab_freq))


taxas_entropia_antes = dict()

for col in renda.select_dtypes(include=['object', 'bool']).columns:
    tab = renda[col].value_counts() / len(renda[col])
    h = calcular_entropia(tab)
    h_max = calcular_entropia_maxima(tab)
    tx_h = (h / h_max) * 100
    taxas_entropia_antes[col] = tx_h
    print('A entropia da variável {} é {:.2f} com seu máximo de entropia sendo {:.2f}'.format(col, h, h_max))
    print('Taxa de Entropia {:.2f}%'.format(tx_h))
    print()








renda.info()





df = renda.dropna(axis=0).drop_duplicates(subset='id_cliente')
df.info()





for col in df.select_dtypes(include=['object', 'bool']).columns:
    tab = df[col].value_counts() / len(df[col])
    h = calcular_entropia(tab)
    h_max = calcular_entropia_maxima(tab)
    tx_h = (h / h_max) * 100
    print('A entropia da variável {} é de {:.2f} e seu máximo de entropia é {:.2f}'.format(col, h, h_max))
    print('Taxa de Entropia Atual {:.2f}%'.format(tx_h))
    print('Taxa de Entropia Anterior {:.2f}%'.format(taxas_entropia_antes[col]))
    print('Diferença: {:.2f}%'.format(tx_h - taxas_entropia_antes[col]))
    print()





df['sexo'] = df['sexo'].map({'F': 1, 'M': 0})
df['posse_de_veiculo'] = df['posse_de_veiculo'].astype(np.int64)
df['posse_de_imovel'] = df['posse_de_imovel'].astype(np.int64)
df['qt_pessoas_residencia'] = df['qt_pessoas_residencia'].astype(np.int64)

df.head(5)





%matplotlib inline
sns.set_style('whitegrid')
sns.heatmap(data=pd.get_dummies(data=df).drop(labels=[col for col in df.columns], errors='ignore', axis=1).corr(), cmap='crest')
plt.show()





corr = pd.get_dummies(data=df).drop(labels=[col for col in df.columns], errors='ignore', axis=1).corr()
values0 = dict()
values = dict()

for col in corr:
    values0[col] = ([(corr[col].index[corr[col] == item].tolist()[0], item) for item in corr[col] if (item > 0.5) or (item < -0.5)])

for key in values0.keys():
    if len(values0[key]) != 1:
        values[key] = [item for item in values0[key] if item[1] != 1.0]

for key in values.keys():
    print('A classe {} tem correlações com:'.format(key))
    for c, v in values[key]:
        print('> {} igual a {}'.format(c, v))
    print()





sns.heatmap(data=pd.get_dummies(data=df).corr(), cmap='crest')
plt.show()





corr = pd.get_dummies(data=df).corr()
values0 = dict()
values = dict()

for col in corr:
    values0[col] = ([(corr[col].index[corr[col] == item].tolist()[0], item) for item in corr[col] if (item > 0.5) or (item < -0.5)])

for key in values0.keys():
    if len(values0[key]) != 1:
        values[key] = [item for item in values0[key] if item[1] != 1.0]

for key in values.keys():
    print('A classe/variável {} tem correlações com:'.format(key))
    for c, v in values[key]:
        print('> {} igual a {}'.format(c, v))
    print()





pd.get_dummies(data=df).assign(renda = df['renda']).corr()['renda']





fig, ax = plt.subplots()
sns.barplot(pd.get_dummies(data=df)
            .drop(labels=[col for col in df.columns], errors='ignore', axis=1)
            .assign(renda = df['renda'])
            .corr()
            .drop('renda', axis=0)['renda'], ax=ax)
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.set_yticks(ticks=np.arange(-.3, 1.1, .1))
ax.set_ylabel('Correlação com Renda')
plt.title('Correlação das Classes com a Variável Renda')
plt.show()





fig, ax = plt.subplots()
sns.barplot(pd.get_dummies(data=df).corr()['renda'].drop('renda', axis=0), ax=ax)
ax.set_xticks(ax.get_xticks())
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.set_yticks(ticks=np.arange(-.3, 1.1, .1))
ax.set_ylabel('Correlação com Renda')
plt.title('Correlação das Variáveis com Renda')
plt.show()





df = df.drop('tipo_residencia', axis=1)
df.head(5)





df = df.drop('data_ref', axis=1).set_index('id_cliente', drop=True)
dummies = pd.get_dummies(df[['tipo_renda', 'educacao', 'estado_civil']], dtype=np.int64)
data = pd.concat([df.drop(['qt_pessoas_residencia', 'qtd_filhos', 'tipo_renda', 'educacao', 'estado_civil'], axis=1), dummies], axis=1)
data.head(5)





vars = []

for col in data:
    if data[col].nunique() == 2:
        tab = data[col].value_counts() / len(data[col])
        print('Taxa de Entropia da Variável {} = {:.2f}%'.format(col, calcular_entropia(tab) / calcular_entropia_maxima(tab) *100))
        print()   
    else:
        vars.append(col)


for col in data:
    if data[col].nunique() == 2:
        tab = data[col].value_counts() / len(data[col])
        if (calcular_entropia(tab) / calcular_entropia_maxima(tab)) > 0.65:
            vars.append(col)
vars


X = data[vars].drop('renda', axis=1)
y = data['renda']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=412)

print(f'Tamanho base de treino: {X_train.shape[0]} amostras\nTamanho base de teste: {X_test.shape[0]} amostras')


reg = DecisionTreeRegressor(random_state=412).fit(X_train, y_train)

path = reg.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas[::10]

regs = list()

for ccp_alpha in ccp_alphas:
    tree = DecisionTreeRegressor(random_state=412, ccp_alpha=ccp_alpha).fit(X_train, y_train)
    regs.append(tree)

train_scores = [reg.score(X_train, y_train) for reg in regs]
test_scores = [reg.score(X_test, y_test) for reg in regs]

print(f'Acurácia na base de treino {max(train_scores)*100:.2f}%')
print(f'Acurácia na base de teste {max(test_scores)*100:.2f}%')


sns.set_style('darkgrid')

fig, ax = plt.subplots()

ax.set_ylabel('Acurácia')
ax.set_xlabel('Alpha')
ax.set_title('Acurácia x Alpha nas Bases')

sns.lineplot(x=ccp_alphas, y=train_scores, drawstyle='steps-post', marker='o', label='Treino', ax=ax)
sns.lineplot(x=ccp_alphas, y=test_scores, drawstyle='steps-post', marker='o', label='Teste', ax=ax)

plt.show()


best_tree = DecisionTreeRegressor(ccp_alpha=ccp_alphas[test_scores.index(max(test_scores))], random_state=412)
best_tree.fit(X_train, y_train)

print('Resultados')
print('='*80)
print(f'Melhor Árvore Treinada sem RandomForest Avaliada na Base de Treino: {best_tree.score(X_train, y_train)*100:.2f}%')
print(f'Melhor Árvore Treinada sem RandomForest Avaliada na Base de Teste: {best_tree.score(X_test, y_test)*100:.2f}%')





regr = RandomForestRegressor(random_state=412, 
                             ccp_alpha=ccp_alphas[test_scores.index(max(test_scores))], 
                             warm_start=True, 
                             n_estimators=500)
regr.fit(X_train, y_train)

print('Resultados')
print('='*80)
print(f'Melhor Árvore Treinada com RandomForest Avaliada na Base de Treino: {regr.score(X_train, y_train)*100:.2f}%')
print(f'Melhor Árvore Treinada com RandomForest Avaliada na Base de Teste: {regr.score(X_test, y_test)*100:.2f}%')








half_data = data.iloc[::2]

hX = half_data[vars].drop('renda', axis=1)
hy = half_data['renda']

X_train0, X_test0, y_train0, y_test0 = train_test_split(hX, hy, test_size=.25, random_state=412)

reg0 = DecisionTreeRegressor(random_state=412).fit(X_train0, y_train0)

path0 = reg0.cost_complexity_pruning_path(X_train0, y_train0)
ccp_alphas0 = path0.ccp_alphas[::10]

regs0 = list()

for ccp_alpha in ccp_alphas0:
    tree = DecisionTreeRegressor(random_state=412, ccp_alpha=ccp_alpha).fit(X_train0, y_train0)
    regs0.append(tree)

train_scores0 = [reg0.score(X_train0, y_train0) for reg in regs0]
test_scores0 = [reg0.score(X_test0, y_test0) for reg in regs0]


regr0 = RandomForestRegressor(random_state=412, 
                              ccp_alpha=ccp_alphas0[test_scores0.index(max(test_scores0))], 
                              warm_start=True, 
                              n_estimators=500)
regr0.fit(X_train0, y_train0)

print('Resultados')
print('='*80)
print(f'Melhor Árvore Treinada com RandomForest Avaliada na Base de Treino: {regr0.score(X_train0, y_train0)*100:.2f}%')
print(f'Melhor Árvore Treinada com RandomForest Avaliada na Base de Teste: {regr0.score(X_test0, y_test0)*100:.2f}%')






